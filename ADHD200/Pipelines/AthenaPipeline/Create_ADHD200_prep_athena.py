# === Create ADHD200_prep_athena.ipynb (enhanced) ===
from nbformat import v4 as nbf_v4, writes
from pathlib import Path

def md(t): return nbf_v4.new_markdown_cell(t)
def code(t): return nbf_v4.new_code_cell(t)

cells=[]

cells += [md(
"# ADHD-200 • Athena-filtered preparation notebook (enhanced)\n\n"
"Готовит **Athena (filtered) time-series** к обучению:\n"
"• per-run z-score, выравнивание до `T_FIX` (окна/паддинг+маска)\n"
"• `manifest_windows.csv` / `aggregate_manifest.csv`\n"
"• (опц.) dFC: оконные FC → KMeans → агрегаты\n"
"• валидатор\n"
"• авто-детект CC200/CC400\n"
"• фильтр по возрастным когортам (12–15, 16–21)\n"
"• экспорт `.tsv.gz` рядом с `.npy`"
)]

cells += [code(
"ATHENA_MODE = True\n"
"T_FIX = 200\n"
"WINDOWS_PER_RUN = 1\n"
"SAVE_WINDOW_NPY = True\n"
"SAVE_WINDOW_TSV = True\n"
"DO_DFC_FEATURES = True\n"
"K_STATES = 5\n"
"SEED = 42\n"
"\n"
"# ВХОД:\n"
"ATHENA_ROOT = '/path/to/AthenaFiltered'   # корень с .1D\n"
"BIDS_BOLD_JSON_ROOT = None                # если есть BIDS с *_bold.json (для TR)\n"
"TR_MAP_CSV = None                         # csv: participant_id,TR (fallback)\n"
"PHENO_CSV = None                          # csv фенотипов, см. PHENO_AGE_COL\n"
"PHENO_AGE_COL = 'Age'\n"
"APPLY_AGE_FILTER = False\n"
"AGE_COHORTS = [(12,15),(16,21)]\n"
)]

cells += [code(
"import numpy as np, pandas as pd, re, json\n"
"from pathlib import Path\n"
"from sklearn.cluster import KMeans\n"
"\n"
"def zscore_time(ts):\n"
"    m = ts.mean(0, keepdims=True); s = ts.std(0, keepdims=True)+1e-8\n"
"    return (ts-m)/s\n"
"\n"
"def choose_windows(ts, T_fix, windows_per_run=1):\n"
"    T,R = ts.shape; win=[]; msk=[]; seg=[]\n"
"    if T>=T_fix:\n"
"        if windows_per_run<=1:\n"
"            st=(T-T_fix)//2; e=st+T_fix\n"
"            win.append(ts[st:e]); msk.append(np.ones(T_fix,dtype=np.float32)); seg.append((st,e))\n"
"        else:\n"
"            step=(T-T_fix)//max(1,windows_per_run-1) if T>T_fix else 0\n"
"            for i in range(windows_per_run):\n"
"                st=i*step; e=st+T_fix\n"
"                if e<=T:\n"
"                    win.append(ts[st:e]); msk.append(np.ones(T_fix,dtype=np.float32)); seg.append((st,e))\n"
"    else:\n"
"        pad=np.zeros((T_fix-T, ts.shape[1]), dtype=ts.dtype)\n"
"        w=np.vstack([ts,pad]); m=np.zeros(T_fix,dtype=np.float32); m[:T]=1\n"
"        win.append(w); msk.append(m); seg.append((0,T))\n"
"    return win, msk, seg\n"
"\n"
"def upper_tri_fc(ts):\n"
"    x=ts-ts.mean(0,keepdims=True); x/=x.std(0,keepdims=True)+1e-8\n"
"    C=np.corrcoef(x, rowvar=False); iu=np.triu_indices_from(C,1)\n"
"    r=C[iu]; z=0.5*np.log((1+r+1e-8)/(1-r+1e-8)); return z.astype(np.float32)\n"
)]

cells += [code(
"def _parse_info_from_filename(fname):\n"
"    stem=Path(fname).stem\n"
"    pid=stem.split('_')[0]\n"
"    m=re.search(r'session_(\\d+).*?rest_(\\d+)', stem); run_id=f'ses{m.group(1)}_rest{m.group(2)}' if m else 'run1'\n"
"    atlas='CC400' if re.search(r'cc400', stem, flags=re.I) else ('CC200' if re.search(r'cc200', stem, flags=re.I) else 'UNKNOWN')\n"
"    return pid, run_id, atlas\n"
"\n"
"def _guess_site_from_path(p: Path):\n"
"    return p.parts[-2] if len(p.parts)>=2 else 'na'\n"
"\n"
"def _find_tr_for_pid(pid, bids_root=None, tr_map_csv=None):\n"
"    if tr_map_csv and Path(tr_map_csv).exists():\n"
"        dftr=pd.read_csv(tr_map_csv); r=dftr[dftr['participant_id'].astype(str)==str(pid)]\n"
"        if len(r):\n"
"            try: return float(r.iloc[0]['TR'])\n"
"            except: pass\n"
"    if bids_root and Path(bids_root).exists():\n"
"        for j in Path(bids_root).rglob(f\"*{pid}*bold.json\"):\n"
"            try:\n"
"                d=json.loads(Path(j).read_text());\n"
"                if 'RepetitionTime' in d: return float(d['RepetitionTime'])\n"
"            except: pass\n"
"    return float('nan')\n"
"\n"
"def _load_pheno(pheno_csv, age_col='Age'):\n"
"    if not pheno_csv or not Path(pheno_csv).exists(): return None\n"
"    df=pd.read_csv(pheno_csv)\n"
"    if 'participant_id' not in df.columns:\n"
"        c=[c for c in df.columns if c.lower() in ('participant_id','subject','sub','id')]\n"
"        if c: df=df.rename(columns={c[0]:'participant_id'})\n"
"        else: raise ValueError('В фенотипах нет participant_id')\n"
"    if age_col not in df.columns: raise ValueError(f'Нет колонки возраста: {age_col}')\n"
"    return df[['participant_id',age_col]].rename(columns={age_col:'Age'})\n"
"\n"
"def _age_in_cohorts(age, cohorts):\n"
"    try: a=float(age)\n"
"    except: return False\n"
"    return any(lo<=a<=hi for lo,hi in cohorts)\n"
"\n"
"def build_run_table(athena_root, bids_root=None, tr_map_csv=None,\n"
"                    pheno_csv=None, apply_age_filter=False, age_cohorts=None,\n"
"                    age_col='Age'):\n"
"    root=Path(athena_root)\n"
"    if not root.exists(): raise FileNotFoundError(f'Не найдена папка: {root}')\n"
"    ph=_load_pheno(pheno_csv, age_col=age_col) if pheno_csv else None\n"
"    rows=[]\n"
"    for p in root.rglob('*TCs.1D'):\n"
"        pid, run_id, atlas=_parse_info_from_filename(p.name)\n"
"        site=_guess_site_from_path(p)\n"
"        TR=_find_tr_for_pid(pid, bids_root=bids_root, tr_map_csv=tr_map_csv)\n"
"        if ph is not None:\n"
"            m=ph[ph['participant_id'].astype(str)==pid]\n"
"            age_val=(m.iloc[0]['Age'] if len(m) else np.nan)\n"
"            if apply_age_filter:\n"
"                cohorts=age_cohorts if age_cohorts is not None else [(12,15),(16,21)]\n"
"                if not _age_in_cohorts(age_val, cohorts):\n"
"                    continue\n"
"        else:\n"
"            age_val=np.nan\n"
"        rows.append({'participant_id':pid,'run_id':run_id,'site':site,'atlas':atlas,'Age':age_val,'TR':TR,'path_1D':str(p)})\n"
"    return pd.DataFrame(rows)\n"
)]

cells += [md("## Подготовка окон (+ экспорт TSV)")]

cells += [code(
"def athena_prepare_runs(run_table: pd.DataFrame, out_root: str):\n"
"    out=Path(out_root); out.mkdir(parents=True, exist_ok=True)\n"
"    rows=[]; all_fc=[]; keys=[]\n"
"    for i,row in run_table.iterrows():\n"
"        pid=str(row['participant_id']); run_id=str(row.get('run_id', f'run{i}'))\n"
"        site=str(row.get('site','na')); atlas=str(row.get('atlas','UNKNOWN'))\n"
"        TR=float(row.get('TR', np.nan)) if pd.notnull(row.get('TR', np.nan)) else np.nan\n"
"\n"
"        # загрузка ряда\n"
"        if isinstance(row.get('npy_path'),str) and Path(row['npy_path']).exists():\n"
"            ts=np.load(row['npy_path']).astype('float32')\n"
"        else:\n"
"            arr=[]\n"
"            with open(row['path_1D'],'r') as f:\n"
"                for ln in f:\n"
"                    ln=ln.strip()\n"
"                    if not ln or ln.startswith('#'): continue\n"
"                    arr.append([float(x) for x in ln.split()])\n"
"            ts=np.asarray(arr,dtype='float32')\n"
"            if ts.shape[0] < ts.shape[1]: ts = ts.T  # (R,T)->(T,R)\n"
"\n"
"        ts_z=zscore_time(ts)\n"
"        wins, masks, segs=choose_windows(ts_z, T_FIX, windows_per_run=WINDOWS_PER_RUN)\n"
"\n"
"        bdir=out/f\"{pid}\"; bdir.mkdir(exist_ok=True)\n"
"        np.save(bdir/f\"{run_id}_zscore.npy\", ts_z.astype('float32'))\n"
"\n"
"        for wi,(w,m,sg) in enumerate(zip(wins,masks,segs),1):\n"
"            wpath=bdir/f\"{run_id}_win{wi}_T{T_FIX}.npy\"; mpath=bdir/f\"{run_id}_win{wi}_T{T_FIX}_mask.npy\"\n"
"            if SAVE_WINDOW_NPY:\n"
"                np.save(wpath,w.astype('float32')); np.save(mpath,m.astype('float32'))\n"
"            tsv_path=bdir/f\"{run_id}_win{wi}_T{T_FIX}.tsv.gz\" if SAVE_WINDOW_TSV else ''\n"
"            if SAVE_WINDOW_TSV:\n"
"                pd.DataFrame(w).to_csv(tsv_path, sep='\\t', index=False, header=False, compression='gzip')\n"
"            rows.append({\n"
"                'participant_id':pid,'run_id':run_id,'site':site,'atlas':atlas,'TR':TR,\n"
"                'win_index':wi,'T_fix':T_FIX,'segments':json.dumps({'start':int(sg[0]),'end':int(sg[1])}),\n"
"                'npy_path':str(wpath),'mask_path':str(mpath),'tsv_path':str(tsv_path)\n"
"            })\n"
"            if DO_DFC_FEATURES:\n"
"                all_fc.append(upper_tri_fc(w)); keys.append((pid,run_id,wi))\n"
"\n"
"    man=pd.DataFrame(rows); man_path=out/'manifest_windows.csv'; man.to_csv(man_path, index=False)\n"
"    agg=(man.groupby('participant_id').agg({'site':'first','atlas':'first','TR':'median','T_fix':'first'}).reset_index())\n"
"    agg_path=out/'aggregate_manifest.csv'; agg.to_csv(agg_path, index=False)\n"
"\n"
"    if DO_DFC_FEATURES and len(all_fc)>=K_STATES:\n"
"        X=np.stack(all_fc,0); km=KMeans(n_clusters=K_STATES, random_state=SEED, n_init='auto')\n"
"        labels=km.fit_predict(X)\n"
"        df_states=pd.DataFrame(keys, columns=['participant_id','run_id','win_index']); df_states['state']=labels\n"
"        feats=[]\n"
"        for pid,dfp in df_states.groupby('participant_id'):\n"
"            seq=dfp.sort_values(['run_id','win_index'])['state'].to_numpy()\n"
"            cnt=np.bincount(seq, minlength=K_STATES)/len(seq)\n"
"            switches=int((seq[1:]!=seq[:-1]).sum()) if len(seq)>1 else 0\n"
"            lens=[]; cur=1\n"
"            for i in range(1,len(seq)):\n"
"                if seq[i]==seq[i-1]: cur+=1\n"
"                else: lens.append(cur); cur=1\n"
"            lens.append(cur); mean_dur=float(np.mean(lens)) if lens else 0.0\n"
"            row={'participant_id':pid,'switches':switches,'mean_dur':mean_dur}\n"
"            for k in range(K_STATES): row[f'frac_state_{k}']=cnt[k]\n"
"            feats.append(row)\n"
"        pd.DataFrame(feats).to_csv(Path(out_root)/'dfc_features.csv', index=False)\n"
"        np.save(Path(out_root)/'dfc_kmeans_centroids.npy', km.cluster_centers_.astype('float32'))\n"
"\n"
"    print('manifest_windows.csv ->', man_path)\n"
"    print('aggregate_manifest.csv ->', agg_path)\n"
"    if DO_DFC_FEATURES and len(all_fc)>=K_STATES:\n"
"        print('dfc_features.csv ->', Path(out_root)/'dfc_features.csv')\n"
"        print('dfc_kmeans_centroids.npy ->', Path(out_root)/'dfc_kmeans_centroids.npy')\n"
)]

cells += [md("## Валидация")]

cells += [code(
"def validate_prepared(out_root):\n"
"    out=Path(out_root); man=out/'manifest_windows.csv'\n"
"    if not man.exists(): print('[ERROR] нет manifest_windows.csv в', out); return\n"
"    df=pd.read_csv(man); problems=0\n"
"    missing_npy=df[~df['npy_path'].apply(lambda p: Path(p).exists())]\n"
"    missing_msk=df[~df['mask_path'].apply(lambda p: Path(p).exists())]\n"
"    if len(missing_npy): problems+=len(missing_npy); print('[WARN] отсутствуют npy:', len(missing_npy))\n"
"    if len(missing_msk): problems+=len(missing_msk); print('[WARN] отсутствуют mask:', len(missing_msk))\n"
"    if 'tsv_path' in df.columns:\n"
"        missing_tsv=df[(df['tsv_path'].astype(str)!='') & (~df['tsv_path'].apply(lambda p: Path(p).exists()))]\n"
"        if len(missing_tsv): problems+=len(missing_tsv); print('[WARN] отсутствуют tsv:', len(missing_tsv))\n"
"    sample=None\n"
"    for p in df['npy_path'].head(5):\n"
"        if Path(p).exists():\n"
"            arr=np.load(p); sample=arr.shape\n"
"            if arr.shape[0]!=T_FIX: print(f'[WARN] {p}: T != T_FIX ({arr.shape[0]} != {T_FIX})')\n"
"    if sample: print('Пример формы ряда:', sample)\n"
"    print('\\nСводка по сайтам:')\n"
"    print(df.groupby('site')['participant_id'].nunique().sort_values(ascending=False))\n"
"    print('\\nСводка по окнам на участника:')\n"
"    print(df.groupby('participant_id')['win_index'].count().describe())\n"
"    dups=df[df.duplicated(['participant_id','run_id','win_index'], keep=False)]\n"
"    if len(dups): problems+=len(dups); print('[WARN] повторяющиеся окна:', len(dups))\n"
"    print('OK' if problems==0 else f'Завершено с предупреждениями: {problems}')\n"
)]

cells += [md("## Мини-прогон")]
cells += [code(
"OUT_ROOT='./athena_prepared'\n"
"# Пример:\n"
"# run_table=build_run_table(ATHENA_ROOT,\n"
"#     bids_root=BIDS_BOLD_JSON_ROOT, tr_map_csv=TR_MAP_CSV,\n"
"#     pheno_csv=PHENO_CSV, apply_age_filter=APPLY_AGE_FILTER,\n"
"#     age_cohorts=AGE_COHORTS, age_col=PHENO_AGE_COL)\n"
"# athena_prepare_runs(run_table, OUT_ROOT)\n"
"# validate_prepared(OUT_ROOT)\n"
)]

nb = nbf_v4.new_notebook(cells=cells, metadata={"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}})
p = Path("ADHD200_prep_athena.ipynb")
p.write_text(writes(nb, version=4), encoding="utf-8")
print("Created:", p.resolve())
